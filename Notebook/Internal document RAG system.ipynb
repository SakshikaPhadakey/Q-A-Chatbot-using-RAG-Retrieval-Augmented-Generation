{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8403769,"sourceType":"datasetVersion","datasetId":5000545},{"sourceId":8403807,"sourceType":"datasetVersion","datasetId":5000576}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-13T18:33:57.089194Z","iopub.execute_input":"2024-05-13T18:33:57.089714Z","iopub.status.idle":"2024-05-13T18:33:57.098897Z","shell.execute_reply.started":"2024-05-13T18:33:57.089672Z","shell.execute_reply":"2024-05-13T18:33:57.097903Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/book-pdf/Introduction to Algorithms 4th Leiserson. Stein .Rivest .Cormen.MIT.Press..pdf\n/kaggle/input/book-pdf/Introductiont o Linear Regression Analysis by DouglasC. Montgomery ElizabethA .PeckG. GeoffreyViningz-.pdf\n/kaggle/input/rag-data/Vidipt Vashist_MA22M025.pdf\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install -q chromadb httpx tldextract sanic llama_index jsonify sentence_transformers","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:33:57.100815Z","iopub.execute_input":"2024-05-13T18:33:57.101147Z","iopub.status.idle":"2024-05-13T18:34:12.558673Z","shell.execute_reply.started":"2024-05-13T18:33:57.101121Z","shell.execute_reply":"2024-05-13T18:34:12.557507Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install langchain==0.1.14","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:34:12.560804Z","iopub.execute_input":"2024-05-13T18:34:12.561185Z","iopub.status.idle":"2024-05-13T18:34:26.233247Z","shell.execute_reply.started":"2024-05-13T18:34:12.561153Z","shell.execute_reply":"2024-05-13T18:34:26.232007Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: langchain==0.1.14 in /opt/conda/lib/python3.10/site-packages (0.1.14)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (0.6.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (1.33)\nRequirement already satisfied: langchain-community<0.1,>=0.0.30 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (0.0.31)\nRequirement already satisfied: langchain-core<0.2.0,>=0.1.37 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (0.1.52)\nRequirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (0.0.1)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (0.1.57)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14) (1.3.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.14) (3.21.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.14) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.14) (2.4)\nRequirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.37->langchain==0.1.14) (23.2)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.14) (3.10.3)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.14) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.14) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.14) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.14) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.14) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.14) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.14) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.14) (3.0.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.14) (1.0.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install langchain-community==0.0.31","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:34:26.234888Z","iopub.execute_input":"2024-05-13T18:34:26.235303Z","iopub.status.idle":"2024-05-13T18:34:39.716545Z","shell.execute_reply.started":"2024-05-13T18:34:26.235261Z","shell.execute_reply":"2024-05-13T18:34:39.715318Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: langchain-community==0.0.31 in /opt/conda/lib/python3.10/site-packages (0.0.31)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community==0.0.31) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain-community==0.0.31) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community==0.0.31) (3.9.1)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain-community==0.0.31) (0.6.4)\nRequirement already satisfied: langchain-core<0.2.0,>=0.1.37 in /opt/conda/lib/python3.10/site-packages (from langchain-community==0.0.31) (0.1.52)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community==0.0.31) (0.1.57)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain-community==0.0.31) (1.26.4)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain-community==0.0.31) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community==0.0.31) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.0.31) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.0.31) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.0.31) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.0.31) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.0.31) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.0.31) (4.0.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.0.31) (3.21.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.0.31) (0.9.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.37->langchain-community==0.0.31) (1.33)\nRequirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.37->langchain-community==0.0.31) (23.2)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.37->langchain-community==0.0.31) (2.5.3)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.0.31) (3.10.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community==0.0.31) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community==0.0.31) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community==0.0.31) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community==0.0.31) (2024.2.2)\nRequirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.0.31) (4.9.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.0.31) (3.0.3)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.37->langchain-community==0.0.31) (2.4)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.37->langchain-community==0.0.31) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.37->langchain-community==0.0.31) (2.14.6)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.0.31) (1.0.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Import dependencies","metadata":{}},{"cell_type":"code","source":"import os\nfrom langchain.llms import HuggingFaceHub\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.document_loaders import DirectoryLoader\nfrom langchain.document_loaders import PyPDFLoader\n\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer\nfrom time import time\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.vectorstores import Chroma\n\nimport glob\nimport textwrap\n\nfrom time import time\n\nimport os\nfrom langchain.llms import HuggingFaceHub\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.document_loaders import DirectoryLoader\nfrom langchain.document_loaders import PyPDFLoader\n\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer\nfrom time import time\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.vectorstores import Chroma","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:34:39.719592Z","iopub.execute_input":"2024-05-13T18:34:39.720067Z","iopub.status.idle":"2024-05-13T18:34:44.607032Z","shell.execute_reply.started":"2024-05-13T18:34:39.720024Z","shell.execute_reply":"2024-05-13T18:34:44.605967Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Loading Documents","metadata":{}},{"cell_type":"code","source":"time_start = time()\nfrom llama_index.core import SimpleDirectoryReader\nreader = SimpleDirectoryReader(input_dir=\"/kaggle/input/rag-data/\", recursive=False)\ndocuments = reader.load_data()\ntime_end = time()\nprint(f\"Loaded {len(documents)} docs\")","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:34:44.609766Z","iopub.execute_input":"2024-05-13T18:34:44.610715Z","iopub.status.idle":"2024-05-13T18:34:49.951844Z","shell.execute_reply.started":"2024-05-13T18:34:44.610675Z","shell.execute_reply":"2024-05-13T18:34:49.950864Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Loaded 25 docs\n","output_type":"stream"}]},{"cell_type":"code","source":"time_duration = time_end - time_start\n# report the duration\nprint(f'Took {time_duration} seconds')\n\ndocuments = [doc.to_langchain_format() for doc in documents]\n#documents\n\ntime_start = time()\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=70)\ntexts = text_splitter.split_documents(documents)\ntime_end = time()\n\ntime_duration = time_end - time_start\n# report the duration\nprint(f'Took {time_duration} seconds')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:34:49.952947Z","iopub.execute_input":"2024-05-13T18:34:49.953241Z","iopub.status.idle":"2024-05-13T18:34:50.956177Z","shell.execute_reply.started":"2024-05-13T18:34:49.953216Z","shell.execute_reply":"2024-05-13T18:34:50.955054Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Took 5.334426164627075 seconds\nTook 0.006772041320800781 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Emedding creation","metadata":{}},{"cell_type":"code","source":"model_name =   \"sentence-transformers/all-mpnet-base-v2\"     #\"\"\nmodel_kwargs = {\"device\": \"cuda\"}\n\nembeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n\ntime_start = time()\n\nvectordb = Chroma.from_documents(documents=texts, embedding=embeddings, collection_metadata={\"hnsw:space\": \"cosine\"}, persist_directory=\"/kaggle/working/chroma_db/docs_cosine\")\n\ntime_end = time()\n\ntime_duration = time_end - time_start\n# report the duration\nprint(f'Took {time_duration} seconds')","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:34:50.957462Z","iopub.execute_input":"2024-05-13T18:34:50.958064Z","iopub.status.idle":"2024-05-13T18:34:59.063088Z","shell.execute_reply.started":"2024-05-13T18:34:50.958035Z","shell.execute_reply":"2024-05-13T18:34:59.062092Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Took 3.345714807510376 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Loading Vector Database","metadata":{}},{"cell_type":"code","source":"\nload_vector_store = Chroma(persist_directory=\"/kaggle/working/chroma_db/docs_cosine\", embedding_function=embeddings)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:34:59.064320Z","iopub.execute_input":"2024-05-13T18:34:59.064885Z","iopub.status.idle":"2024-05-13T18:34:59.075709Z","shell.execute_reply.started":"2024-05-13T18:34:59.064857Z","shell.execute_reply":"2024-05-13T18:34:59.074877Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Post processing","metadata":{}},{"cell_type":"code","source":"def extract_unique_dicts(list_of_dicts):\n    unique_dicts = []\n    seen_dicts = set()\n\n    for d in list_of_dicts:\n        # Convert the dictionary to a frozenset of items (since dictionaries are not hashable)\n        dict_representation = frozenset(d.items())\n\n        # Check if the dictionary representation is unique\n        if dict_representation not in seen_dicts:\n            unique_dicts.append(d)\n            seen_dicts.add(dict_representation)\n\n    return unique_dicts\n\ndef vector_search_source(query ):\n    docs = load_vector_store.similarity_search_with_score(query=query, k=5)\n    \n    print(\"Sources:\")\n    src = []\n    for meta in docs:\n        #print(list(list(list(meta)[0])[0])[1]) # content\n        keys = ['page_label','file_name'] # \"file_path\",\"file_type\" ,\"creation_date\"\n        src.append(dict(filter(lambda item:item[0] in keys , list(list(list(meta)[0])[1])[1].items()))) # source\n\n    sources = extract_unique_dicts(src)\n    sources = filter(None,sources)\n\n    for meta in sources:\n        print(meta)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:34:59.076973Z","iopub.execute_input":"2024-05-13T18:34:59.077256Z","iopub.status.idle":"2024-05-13T18:34:59.086586Z","shell.execute_reply.started":"2024-05-13T18:34:59.077231Z","shell.execute_reply":"2024-05-13T18:34:59.085628Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Example","metadata":{}},{"cell_type":"code","source":"retriever = load_vector_store.as_retriever(search_kwargs = {\"k\": 3} )\nretriever.get_relevant_documents(\"what is RAG\" )\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:34:59.089853Z","iopub.execute_input":"2024-05-13T18:34:59.090153Z","iopub.status.idle":"2024-05-13T18:34:59.539926Z","shell.execute_reply.started":"2024-05-13T18:34:59.090129Z","shell.execute_reply":"2024-05-13T18:34:59.538975Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n  warn_deprecated(\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[Document(page_content='6 \\n  \\nLIST O F FIGURES  \\n \\nFIGURE 1 OVERVIEW OF RAG  SYSTEM  ................................ ................................ ................................ ................................ .........................  9 \\nFIGURE 2 RAG -BASED SYSTEM  ................................ ................................ ................................ ................................ ........................  10 \\nFIGURE 3 BENEFITS FOR RAG  ................................ ................................ ................................ ................................ ...........................  11', metadata={'creation_date': '2024-05-13', 'file_name': 'Vidipt Vashist_MA22M025.pdf', 'file_path': '/kaggle/input/rag-data/Vidipt Vashist_MA22M025.pdf', 'file_size': 804338, 'file_type': 'application/pdf', 'last_modified_date': '2024-05-13', 'page_label': '6'}),\n Document(page_content='6 \\n  \\nLIST O F FIGURES  \\n \\nFIGURE 1 OVERVIEW OF RAG  SYSTEM  ................................ ................................ ................................ ................................ .........................  9 \\nFIGURE 2 RAG -BASED SYSTEM  ................................ ................................ ................................ ................................ ........................  10 \\nFIGURE 3 BENEFITS FOR RAG  ................................ ................................ ................................ ................................ ...........................  11', metadata={'creation_date': '2024-05-13', 'file_name': 'Vidipt Vashist_MA22M025.pdf', 'file_path': '/kaggle/input/rag-data/Vidipt Vashist_MA22M025.pdf', 'file_size': 804338, 'file_type': 'application/pdf', 'last_modified_date': '2024-05-13', 'page_label': '6'}),\n Document(page_content='6 \\n  \\nLIST O F FIGURES  \\n \\nFIGURE 1 OVERVIEW OF RAG  SYSTEM  ................................ ................................ ................................ ................................ .........................  9 \\nFIGURE 2 RAG -BASED SYSTEM  ................................ ................................ ................................ ................................ ........................  10 \\nFIGURE 3 BENEFITS FOR RAG  ................................ ................................ ................................ ................................ ...........................  11', metadata={'creation_date': '2024-05-13', 'file_name': 'Vidipt Vashist_MA22M025.pdf', 'file_path': '/kaggle/input/rag-data/Vidipt Vashist_MA22M025.pdf', 'file_size': 804338, 'file_type': 'application/pdf', 'last_modified_date': '2024-05-13', 'page_label': '6'})]"},"metadata":{}}]},{"cell_type":"markdown","source":"## LLM Loading","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token=\"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\")","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:34:59.541085Z","iopub.execute_input":"2024-05-13T18:34:59.541386Z","iopub.status.idle":"2024-05-13T18:34:59.598914Z","shell.execute_reply.started":"2024-05-13T18:34:59.541358Z","shell.execute_reply":"2024-05-13T18:34:59.597876Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import (\n  AutoTokenizer,\n  AutoModelForCausalLM,\n  BitsAndBytesConfig,\n  pipeline\n)\n\nfrom torch import cuda, bfloat16\nimport transformers\n\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\nmodel_name =   \"mistralai/Mistral-7B-Instruct-v0.2\"\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\nmodel_name , #\"falcon/rw-1b-instruct\",\n   trust_remote_code=True,\ntorch_dtype=\"auto\"\n)\nmodel.eval()\nmodel.to('cuda:0')\nprint(f\"Model loaded on {device}\")\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_name) #\"falcon/tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:34:59.600245Z","iopub.execute_input":"2024-05-13T18:34:59.600563Z","iopub.status.idle":"2024-05-13T18:36:07.795969Z","shell.execute_reply.started":"2024-05-13T18:34:59.600536Z","shell.execute_reply":"2024-05-13T18:36:07.794898Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"2024-05-13 18:35:00.066292: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-13 18:35:00.066365: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-13 18:35:00.067916: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e2bc3ce10ee451f96183a1d7aea717f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d05c038341704f1e88d2578bcf77b93d"}},"metadata":{}},{"name":"stdout","text":"Model loaded on cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import StoppingCriteria, StoppingCriteriaList\n\n# gpt-j-6b is trained to add \"<|endoftext|>\" at the end of generations\nstop_token_ids = tokenizer.convert_tokens_to_ids([\"<|endoftext|>\"])\n\n# define custom stopping criteria object\nclass StopOnTokens(StoppingCriteria):\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        for stop_id in stop_token_ids:\n            if input_ids[0][-1] == stop_id:\n                return True\n        return False\n\nstopping_criteria = StoppingCriteriaList([StopOnTokens()])\nstopping_criteria","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:36:07.797551Z","iopub.execute_input":"2024-05-13T18:36:07.798439Z","iopub.status.idle":"2024-05-13T18:36:07.807773Z","shell.execute_reply.started":"2024-05-13T18:36:07.798399Z","shell.execute_reply":"2024-05-13T18:36:07.806743Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"[<__main__.StopOnTokens at 0x7988f08f3b20>]"},"metadata":{}}]},{"cell_type":"code","source":"generate_text = transformers.pipeline(\n    model=model, tokenizer=tokenizer,\n    return_full_text=True,  # langchain expects the full text\n    task='text-generation',\n    device='cuda:0',\n    # we pass model parameters here too\n    stopping_criteria=stopping_criteria,  # without this model will ramble\n    temperature=0.01,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n    top_p=0.15,  # select from top tokens whose probability add up to 15%\n    top_k=0,  # select from top 0 tokens (because zero, relies on top_p)\n    max_new_tokens=512,  # mex number of tokens to generate in the output\n    repetition_penalty=1.1 , # without this output begins repeating\n    do_sample=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:36:07.809343Z","iopub.execute_input":"2024-05-13T18:36:07.810253Z","iopub.status.idle":"2024-05-13T18:36:07.829567Z","shell.execute_reply.started":"2024-05-13T18:36:07.810223Z","shell.execute_reply":"2024-05-13T18:36:07.828691Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from langchain import PromptTemplate, LLMChain\nfrom langchain.llms import HuggingFacePipeline\n\nllm = HuggingFacePipeline(pipeline=generate_text)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:36:07.830751Z","iopub.execute_input":"2024-05-13T18:36:07.831099Z","iopub.status.idle":"2024-05-13T18:36:07.836872Z","shell.execute_reply.started":"2024-05-13T18:36:07.831072Z","shell.execute_reply":"2024-05-13T18:36:07.835893Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"prompt_template = \"\"\"\nDon't try to make up an answer, if you don't know just say that you don't know.\nAnswer in the same language the question was asked.\nUse only the following pieces of context to answer the question at the end.\n\n{context}\n\nQuestion: {question}\n\n\nAnswer:\"\"\"\n\nPROMPT = PromptTemplate(\n    template = prompt_template, \n    input_variables = [\"context\", \"question\" ]\n)\n\nfrom langchain import PromptTemplate, LLMChain\nllm_chain = LLMChain(prompt=PROMPT, llm=llm)\nllm_chain","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:37:21.178328Z","iopub.execute_input":"2024-05-13T18:37:21.179479Z","iopub.status.idle":"2024-05-13T18:37:21.187976Z","shell.execute_reply.started":"2024-05-13T18:37:21.179442Z","shell.execute_reply":"2024-05-13T18:37:21.186886Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"\\nDon't try to make up an answer, if you don't know just say that you don't know.\\nAnswer in the same language the question was asked.\\nUse only the following pieces of context to answer the question at the end.\\n\\n{context}\\n\\nQuestion: {question}\\n\\n\\nAnswer:\"), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7988f08f1450>))"},"metadata":{}}]},{"cell_type":"markdown","source":"## Q/A LLM","metadata":{}},{"cell_type":"code","source":"from langchain.chains import RetrievalQA\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:38:08.920402Z","iopub.execute_input":"2024-05-13T18:38:08.920811Z","iopub.status.idle":"2024-05-13T18:38:08.926235Z","shell.execute_reply.started":"2024-05-13T18:38:08.920782Z","shell.execute_reply":"2024-05-13T18:38:08.925074Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"retriever = load_vector_store.as_retriever(search_kwargs = {\"k\": 3})\n\nqa_chain_without_mem = RetrievalQA.from_chain_type(\n    llm = llm,\n    chain_type = \"stuff\", # map_reduce, map_rerank, stuff, refine\n    retriever = retriever, \n    #chain_type_kwargs = {\"prompt\": PROMPT},\n    return_source_documents = True,\n    verbose = False\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:49:07.402654Z","iopub.execute_input":"2024-05-13T18:49:07.403668Z","iopub.status.idle":"2024-05-13T18:49:07.409891Z","shell.execute_reply.started":"2024-05-13T18:49:07.403626Z","shell.execute_reply":"2024-05-13T18:49:07.408710Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"## Post processing","metadata":{}},{"cell_type":"code","source":"def extract_unique_dicts(list_of_dicts):\n    unique_dicts = []\n    seen_dicts = set()\n\n    for d in list_of_dicts:\n        # Convert the dictionary to a frozenset of items (since dictionaries are not hashable)\n        dict_representation = frozenset(d.items())\n\n        # Check if the dictionary representation is unique\n        if dict_representation not in seen_dicts:\n            unique_dicts.append(d)\n            seen_dicts.add(dict_representation)\n\n    return unique_dicts\n\ndef qa_post_processing(raw_result):\n\n    print(\"Answer\")\n    \n    context = raw_result['result']\n    start_index = context.find(\"Helpful Answer:\")\n    extracted_string = context[start_index:].strip().replace(\"Helpful Answer: \" , \"\")\n    print(extracted_string)\n    \n    print(\" \")\n    print(\"Sources:\")\n    src=[]\n    for meta in raw_result['source_documents']:\n        keys = ['page_label','file_name'] # \"file_path\",\"file_type\" ,\"creation_date\"\n        src.append(dict(filter(lambda item:item[0] in keys , list(list(meta)[1])[1].items()))) # source\n\n    sources = extract_unique_dicts(src)\n    sources = filter(None,sources)\n\n    for meta in sources:\n        print(meta)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:49:08.382772Z","iopub.execute_input":"2024-05-13T18:49:08.383917Z","iopub.status.idle":"2024-05-13T18:49:08.392776Z","shell.execute_reply.started":"2024-05-13T18:49:08.383877Z","shell.execute_reply":"2024-05-13T18:49:08.391651Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"## Example","metadata":{}},{"cell_type":"code","source":"query1 = \"what is problem statement of project\"\nresult1 = qa_chain_without_mem(query1)\nqa_post_processing(result1)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:49:11.962692Z","iopub.execute_input":"2024-05-13T18:49:11.963570Z","iopub.status.idle":"2024-05-13T18:49:21.242655Z","shell.execute_reply.started":"2024-05-13T18:49:11.963534Z","shell.execute_reply":"2024-05-13T18:49:21.241671Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Answer\nThe problem statement can be found in Chapter 1, section 1.1 of the document. It states that there is a need for a system that can automatically generate arguments for and against a given topic based on retrieved information from various sources. This is a common task in debates, legal proceedings, and other situations where it is important to have a well-rounded understanding of all sides of an issue. However, current systems are limited in their ability to effectively retrieve and summarize relevant information, let alone generate persuasive arguments based on that information. Therefore, the goal of this project is to develop a Retrieval Argument Generation (RAG) system that can efficiently and accurately retrieve information from multiple sources, summarize it, and generate arguments for and against a given topic.\n \nSources:\n{'file_name': 'Vidipt Vashist_MA22M025.pdf', 'page_label': '5'}\n","output_type":"stream"}]},{"cell_type":"code","source":"query2 = \"benifits of rag\"\nresult2 = qa_chain_without_mem(query2)\nqa_post_processing(result2)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T18:46:11.203928Z","iopub.execute_input":"2024-05-13T18:46:11.204721Z","iopub.status.idle":"2024-05-13T18:46:27.139126Z","shell.execute_reply.started":"2024-05-13T18:46:11.204676Z","shell.execute_reply":"2024-05-13T18:46:27.138077Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Answer\nRAG (Rapid Annotation and Grading) is a technology used for automated analysis, summarization, and grading of text-based content. It offers several benefits across various domains such as education, legal research, document review, and healthcare. Here are some key advantages of using RAG:\n\n1. Enhanced Learning and Comprehension: RAG facilitates more effective learning by providing answers, explanations, and additional context based on textbooks and reference materials. This helps students understand complex concepts better and faster.\n2. Streamlined Legal Research and Document Review: Legal professionals can use RAG models to efficiently summarize statutes, case law, and other legal documents, saving time and improving accuracy.\n3. Improved Medical Diagnosis and Healthcare: In the healthcare domain, RAG models provide doctors and medical professionals with access to the latest medical literature and clinical information, enabling them to make informed decisions and improve patient care.\n\nAdditional Context: RAG uses natural language processing (NLP) techniques and machine learning algorithms to analyze and grade text-based content. It can be integrated into various platforms like learning management systems, document management systems, and electronic health records (EHRs). By automating the process of analyzing and grading text, RAG saves time, reduces errors, and improves overall productivity.\n \nSources:\n{'file_name': 'Vidipt Vashist_MA22M025.pdf', 'page_label': '12'}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}