{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-13T15:13:30.818367Z","iopub.execute_input":"2024-05-13T15:13:30.818743Z","iopub.status.idle":"2024-05-13T15:13:31.692154Z","shell.execute_reply.started":"2024-05-13T15:13:30.818713Z","shell.execute_reply":"2024-05-13T15:13:31.691113Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"pip install -q chromadb httpx tldextract sanic llama_index jsonify sentence_transformers","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:13:31.694155Z","iopub.execute_input":"2024-05-13T15:13:31.695091Z","iopub.status.idle":"2024-05-13T15:14:22.471223Z","shell.execute_reply.started":"2024-05-13T15:13:31.695065Z","shell.execute_reply":"2024-05-13T15:14:22.470099Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 29.0.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install langchain==0.1.14","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:14:22.472456Z","iopub.execute_input":"2024-05-13T15:14:22.472746Z","iopub.status.idle":"2024-05-13T15:14:40.527307Z","shell.execute_reply.started":"2024-05-13T15:14:22.472719Z","shell.execute_reply":"2024-05-13T15:14:40.526170Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting langchain==0.1.14\n  Downloading langchain-0.1.14-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (0.6.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (1.33)\nCollecting langchain-community<0.1,>=0.0.30 (from langchain==0.1.14)\n  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\nCollecting langchain-core<0.2.0,>=0.1.37 (from langchain==0.1.14)\n  Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain==0.1.14)\n  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain==0.1.14)\n  Downloading langsmith-0.1.57-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.14) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.14) (1.3.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.14) (3.21.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.14) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.14) (2.4)\nCollecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.37->langchain==0.1.14)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.14) (3.10.3)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.14) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.14) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.14) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.14) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.14) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.14) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.14) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.14) (3.0.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.14) (1.0.0)\nDownloading langchain-0.1.14-py3-none-any.whl (812 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.8/812.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\nDownloading langsmith-0.1.57-py3-none-any.whl (121 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.0/121.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, langsmith, langchain-core, langchain-text-splitters, langchain-community, langchain\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.9.3 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.1.14 langchain-community-0.0.38 langchain-core-0.1.52 langchain-text-splitters-0.0.1 langsmith-0.1.57 packaging-23.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install langchain-community==0.0.31","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:14:40.529727Z","iopub.execute_input":"2024-05-13T15:14:40.530087Z","iopub.status.idle":"2024-05-13T15:14:56.459071Z","shell.execute_reply.started":"2024-05-13T15:14:40.530056Z","shell.execute_reply":"2024-05-13T15:14:56.457919Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting langchain-community==0.0.31\n  Downloading langchain_community-0.0.31-py3-none-any.whl.metadata (8.4 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community==0.0.31) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain-community==0.0.31) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community==0.0.31) (3.9.1)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain-community==0.0.31) (0.6.4)\nRequirement already satisfied: langchain-core<0.2.0,>=0.1.37 in /opt/conda/lib/python3.10/site-packages (from langchain-community==0.0.31) (0.1.52)\nRequirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community==0.0.31) (0.1.57)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain-community==0.0.31) (1.26.4)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain-community==0.0.31) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community==0.0.31) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.0.31) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.0.31) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.0.31) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.0.31) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.0.31) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.0.31) (4.0.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.0.31) (3.21.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.0.31) (0.9.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.37->langchain-community==0.0.31) (1.33)\nRequirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.37->langchain-community==0.0.31) (23.2)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.37->langchain-community==0.0.31) (2.5.3)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.0.31) (3.10.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community==0.0.31) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community==0.0.31) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community==0.0.31) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community==0.0.31) (2024.2.2)\nRequirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.0.31) (4.9.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.0.31) (3.0.3)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.37->langchain-community==0.0.31) (2.4)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.37->langchain-community==0.0.31) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.37->langchain-community==0.0.31) (2.14.6)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.0.31) (1.0.0)\nDownloading langchain_community-0.0.31-py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: langchain-community\n  Attempting uninstall: langchain-community\n    Found existing installation: langchain-community 0.0.38\n    Uninstalling langchain-community-0.0.38:\n      Successfully uninstalled langchain-community-0.0.38\nSuccessfully installed langchain-community-0.0.31\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Import dependency","metadata":{}},{"cell_type":"code","source":"import concurrent.futures\nimport json\nimport os\nimport re\nimport requests\nimport traceback\nimport httpx\nfrom typing import AsyncGenerator\nimport urllib.parse\n# import trafilatura\n# from trafilatura import bare_extraction\nimport tldextract\nfrom concurrent.futures import ThreadPoolExecutor\nfrom urllib.parse import urlparse\n\nimport sanic\nfrom sanic import Sanic\nimport sanic.exceptions\nfrom sanic.exceptions import HTTPException, InvalidUsage\n\nimport os\n# from dotenv import load_dotenv\nfrom langchain.llms import HuggingFaceHub\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import SentenceTransformerEmbeddings\n\nimport os\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.document_loaders import DirectoryLoader\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.vectorstores import Chroma\n\nimport pandas as pd\n\nimport os\nfrom langchain.llms import HuggingFaceHub\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.document_loaders import DirectoryLoader\nfrom langchain.document_loaders import PyPDFLoader\n\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer\nfrom time import time\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.vectorstores import Chroma\n\nimport glob\nimport textwrap\n\nfrom llama_index.core import SimpleDirectoryReader\n\nfrom torch import cuda, bfloat16\nimport transformers\nimport os\nfrom langchain.llms import HuggingFaceHub\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.document_loaders import DirectoryLoader\nfrom langchain.document_loaders import PyPDFLoader\nimport jsonify\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer\nfrom time import time\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.vectorstores import Chroma","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:14:56.460513Z","iopub.execute_input":"2024-05-13T15:14:56.460833Z","iopub.status.idle":"2024-05-13T15:15:05.905277Z","shell.execute_reply.started":"2024-05-13T15:14:56.460805Z","shell.execute_reply":"2024-05-13T15:15:05.904509Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Web Search","metadata":{}},{"cell_type":"code","source":"\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\nGOOGLE_SEARCH_ENDPOINT = \"https://customsearch.googleapis.com/customsearch/v1\"\n# Specify the number of references from the search engine you want to use.\n# 8 is usually a good number.\nREFERENCE_COUNT = 8\n\n# Specify the default timeout for the search engine. If the search engine\n# does not respond within this time, we will return an error.\nDEFAULT_SEARCH_ENGINE_TIMEOUT = 5\n\n# 默认记录的对话历史长度\nMAX_HISTORY_LEN = 10\n\ndef search_with_google(query: str, subscription_key: str, cx: str):\n    \"\"\"\n    Search with google and return the contexts.\n    \"\"\"\n    params = {\n        \"key\": subscription_key,\n        \"cx\": cx,\n        \"q\": query,\n        \"num\": REFERENCE_COUNT,\n    }\n    response = requests.get(\n        GOOGLE_SEARCH_ENDPOINT, params=params, timeout=DEFAULT_SEARCH_ENGINE_TIMEOUT\n    )\n    if not response.ok:\n        raise HTTPException(\"Search engine error.\")\n    json_content = response.json()\n    try:\n        contexts = json_content[\"items\"][:REFERENCE_COUNT]\n        for item in contexts:\n            item[\"name\"] = item[\"title\"]\n            item[\"url\"] = item[\"link\"]\n    except KeyError:\n        return []\n    return contexts\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:15:05.906364Z","iopub.execute_input":"2024-05-13T15:15:05.906902Z","iopub.status.idle":"2024-05-13T15:15:05.964208Z","shell.execute_reply.started":"2024-05-13T15:15:05.906874Z","shell.execute_reply":"2024-05-13T15:15:05.963372Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef extract_url_content(url):\n    try:\n        # Send a GET request to the URL to fetch the HTML content\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for 4xx or 5xx status codes\n\n        # Parse the HTML content using BeautifulSoup\n        soup = BeautifulSoup(response.text, 'html.parser')\n\n        # Extract the main text content from the parsed HTML\n        text_content = soup.get_text()\n\n        return text_content\n    except Exception as e:\n        #print(f\"Error occurred while extracting content from {url}: {e}\")\n        return None\n\n# Example usage:\n# url = \"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/discussions/15\"  # Replace with the URL you want to extract content from\n# content = extract_url_content(url)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:15:05.965261Z","iopub.execute_input":"2024-05-13T15:15:05.965531Z","iopub.status.idle":"2024-05-13T15:15:06.131490Z","shell.execute_reply.started":"2024-05-13T15:15:05.965508Z","shell.execute_reply":"2024-05-13T15:15:06.130748Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def extract_url(ls):\n    url =[]\n    for ele in ls:\n        url.append(ele['url'])\n    return url\n\ndef extract_content(url):\n    content=[]\n    for u in url:\n        content.append(extract_url_content(u))\n    return content\n\nmodel_name =   \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"     #\"sentence-transformers/all-mpnet-base-v2\"\nmodel_kwargs = {\"device\": \"cuda:0\"}\n\nembeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:15:06.132689Z","iopub.execute_input":"2024-05-13T15:15:06.133435Z","iopub.status.idle":"2024-05-13T15:15:17.012274Z","shell.execute_reply.started":"2024-05-13T15:15:06.133400Z","shell.execute_reply":"2024-05-13T15:15:17.011411Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0941d758cfe64d44880d8b9a728a1b70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e1b3ad60bed48a4a07878d33f267587"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.13k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e47ce34ae8f47db80e1142c49940daf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d99a958bd18f40448aaef409c1d478fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/723 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e8588b5b1f54cf5a4454f3d758d0a7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ded4b448fade43e897e2d22434f1929c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/402 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e744fc9dec104db78096c1a41ada3c63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"703dec3764044a7891871dce1152a10e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f452dbb80b04e32bfda5cb9fac89b68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4303aab6e1344a86b63a53ab8e51e8c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55ca3f822aea43388b74d9c05221d261"}},"metadata":{}}]},{"cell_type":"code","source":"def web_scrap(query):\n    web_search = search_with_google(query , 'AIzaSyA8bFHr-srkVkYYG1s5QB2lAyUQbg7OCl4' , '45c4d28eb7e8e4c93')\n    url = extract_url(web_search)\n    content = extract_content(url)\n    df = pd.DataFrame({'content': content, 'url': url})\n    df['content'] = df['content'].str.replace('\\n', '')\n    #df['content'] = df['content'] + 'LINK: ' + df['url']\n    df.to_csv('data.csv', index=False)\n\n    print(\"Search complete\")\n    #print(\"LINK: \" , list(df['url']))\n\n\n    reader = SimpleDirectoryReader(input_files=[\"/kaggle/working/data.csv\"])\n    documents = reader.load_data()\n    documents = [doc.to_langchain_format() for doc in documents]\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n    texts = text_splitter.split_documents(documents)\n    vectordb = Chroma.from_documents(documents=texts, embedding=embeddings, collection_metadata={\"hnsw:space\": \"cosine\"}, persist_directory=\"/kaggle/working/chroma_db/content\")\n    print(\"Created embedding\")\n\n    load_vector_store = Chroma(persist_directory=\"/kaggle/working/chroma_db/content\", embedding_function=embeddings)\n    return  load_vector_store.as_retriever(search_kwargs = {\"k\": 3} )","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:15:17.013563Z","iopub.execute_input":"2024-05-13T15:15:17.014270Z","iopub.status.idle":"2024-05-13T15:15:17.026431Z","shell.execute_reply.started":"2024-05-13T15:15:17.014235Z","shell.execute_reply":"2024-05-13T15:15:17.025037Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Example\nretriever = web_scrap('who is srk')\nretriever.get_relevant_documents(\"srk \")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:15:17.030894Z","iopub.execute_input":"2024-05-13T15:15:17.031204Z","iopub.status.idle":"2024-05-13T15:15:28.466923Z","shell.execute_reply.started":"2024-05-13T15:15:17.031180Z","shell.execute_reply":"2024-05-13T15:15:28.465872Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Search complete\nCreated embedding\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n  warn_deprecated(\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"[Document(page_content='them seems larger than the structures that hold them, pure, unvarnished.             SRK’s 2000s-era six pack.      Photo: Eros Entertainment/Everett CollectionLike so many figures who capture mass attention, Shah Rukh Khan can seem to have been born to throw matters of identity into question. The son of a Pathan father from Afghanistan and a Hyderabadi mother, he grew up in Delhi, a middle-class boy who attended a prestigious local private school. That history helps explain his appeal, or so', metadata={'creation_date': '2024-05-13', 'file_name': 'data.csv', 'file_path': '/kaggle/working/data.csv', 'file_size': 159922, 'file_type': 'text/csv', 'last_modified_date': '2024-05-13'}),\n Document(page_content='12 June 2014.^ Gupta, Priya (4 August 2013). \"How SRK\\'s Pathan father fell in love with his South Indian mother\". The Times of India. Archived from the original on 5 August 2013. Retrieved 24 October 2013.^ \"Happy Birthday Shah Rukh Khan: Why SRK is \\'Half Hyderabadi\\'?\". The Siasat Daily. 2 November 2020. Archived from the original on 17 June 2021. Retrieved 9 January 2021. Khan\\'s paternal grandfather, Meer Jan Muhammad Khan, was an ethnic Pashtun (Pathan) from Afghanistan^ Mardomi interviews', metadata={'creation_date': '2024-05-13', 'file_name': 'data.csv', 'file_path': '/kaggle/working/data.csv', 'file_size': 159922, 'file_type': 'text/csv', 'last_modified_date': '2024-05-13'}),\n Document(page_content='it about SRK? He is at once the biggest and most versatile of Bollywood heroes, a star said to command a larger fan following than any other working actor in the world today, a man who does psychological thriller, screwball comedy, earnest romance, ditzy rom-com, serious biopic, and, lately, testosterone-fueled action. He’s said to be five-feet-seven, shorter than some of his female co-stars, seemingly still in full possession of his springy hair, and is physically muscular yet lithe, with a', metadata={'creation_date': '2024-05-13', 'file_name': 'data.csv', 'file_path': '/kaggle/working/data.csv', 'file_size': 159922, 'file_type': 'text/csv', 'last_modified_date': '2024-05-13'})]"},"metadata":{}}]},{"cell_type":"markdown","source":"## Model loading","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token=\"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\")","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:15:28.468540Z","iopub.execute_input":"2024-05-13T15:15:28.469124Z","iopub.status.idle":"2024-05-13T15:15:28.514034Z","shell.execute_reply.started":"2024-05-13T15:15:28.469099Z","shell.execute_reply":"2024-05-13T15:15:28.513116Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import (\n  AutoTokenizer,\n  AutoModelForCausalLM,\n  BitsAndBytesConfig,\n  pipeline\n)\n\nfrom torch import cuda, bfloat16\nimport transformers\n\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\nmodel_name =   \"mistralai/Mistral-7B-Instruct-v0.2\"\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\nmodel_name , #\"falcon/rw-1b-instruct\",\n   trust_remote_code=True,\ntorch_dtype=\"auto\"\n)\nmodel.eval()\nmodel.to('cuda:0')\nprint(f\"Model loaded on {device}\")\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_name) #\"falcon/tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:15:28.515141Z","iopub.execute_input":"2024-05-13T15:15:28.515436Z","iopub.status.idle":"2024-05-13T15:18:02.985728Z","shell.execute_reply.started":"2024-05-13T15:15:28.515411Z","shell.execute_reply":"2024-05-13T15:18:02.983975Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"2024-05-13 15:15:30.156056: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-13 15:15:30.156190: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-13 15:15:30.280295: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b997e40909f941caa0275494307bc57b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"383293e997ba48f0b1e1922812117632"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"683e919595804bf0a4f2f9c3fb8e3a2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"650e6639c2bb492996713d54908c31f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9398cbb06e27469692c8eefb11e86c7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cc18faf8881463db327d77d4a0003f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"569bd1108ab74b00bd749abe05b7f13f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18a5858be7444b1cb1d785bfc79b9ea8"}},"metadata":{}},{"name":"stdout","text":"Model loaded on cuda:0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32a617bf472b4f19bcdfc34b68aeefcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20063d9a012846b1b0b48e3090103419"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"776b931f89754f1680c769a91127e6b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47695e58f7634ccd8aa53fe06ca810d3"}},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom transformers import StoppingCriteria, StoppingCriteriaList\n\n# gpt-j-6b is trained to add \"<|endoftext|>\" at the end of generations\nstop_token_ids = tokenizer.convert_tokens_to_ids([\"<|endoftext|>\"])\n\n# define custom stopping criteria object\nclass StopOnTokens(StoppingCriteria):\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        for stop_id in stop_token_ids:\n            if input_ids[0][-1] == stop_id:\n                return True\n        return False\n\nstopping_criteria = StoppingCriteriaList([StopOnTokens()])\nstopping_criteria","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:18:02.988933Z","iopub.execute_input":"2024-05-13T15:18:02.989340Z","iopub.status.idle":"2024-05-13T15:18:03.000066Z","shell.execute_reply.started":"2024-05-13T15:18:02.989309Z","shell.execute_reply":"2024-05-13T15:18:02.999119Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[<__main__.StopOnTokens at 0x7aa032e6baf0>]"},"metadata":{}}]},{"cell_type":"code","source":"generate_text = transformers.pipeline(\n    model=model, tokenizer=tokenizer,\n    return_full_text=True,  # langchain expects the full text\n    task='text-generation',\n    device='cuda:0',\n    # we pass model parameters here too\n    stopping_criteria=stopping_criteria,  # without this model will ramble\n    temperature=0.01,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n    top_p=0.15,  # select from top tokens whose probability add up to 15%\n    top_k=0,  # select from top 0 tokens (because zero, relies on top_p)\n    max_new_tokens=512,  # mex number of tokens to generate in the output\n    repetition_penalty=1.1 , # without this output begins repeating\n    do_sample=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:18:03.001503Z","iopub.execute_input":"2024-05-13T15:18:03.001835Z","iopub.status.idle":"2024-05-13T15:18:03.035075Z","shell.execute_reply.started":"2024-05-13T15:18:03.001802Z","shell.execute_reply":"2024-05-13T15:18:03.034252Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from langchain import PromptTemplate, LLMChain\nfrom langchain.llms import HuggingFacePipeline\n\nllm = HuggingFacePipeline(pipeline=generate_text)\n\n#llm_chain = LLMChain(llm=llm, prompt=prompt)\n\n# with memory\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.memory import ConversationBufferMemory\n\nmemory = ConversationBufferMemory(\n        memory_key='chat_history', return_messages=True, output_key='answer')","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:18:03.036112Z","iopub.execute_input":"2024-05-13T15:18:03.036370Z","iopub.status.idle":"2024-05-13T15:18:03.062806Z","shell.execute_reply.started":"2024-05-13T15:18:03.036348Z","shell.execute_reply":"2024-05-13T15:18:03.061942Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#----\n\n# def find_link(text , df):\n#     # Assuming df is your DataFrame\n#     search_string = text.page_content\n\n#     # Find the row where the content column contains the search string\n#     filtered_df = df[df['content'].notna() & df['content'].str.contains(search_string, na=False , regex=False)]\n\n#     # Assuming there's only one row with the search string, you can extract its URL\n#     if not filtered_df.empty:\n#         url = filtered_df['url'].iloc[0]  # Extract URL from the first row\n#         return url\n\ndef qa_post_processing_mem(raw_result):\n    df= pd.read_csv(\"/kaggle/working/data.csv\")\n    print(\"Answer\")\n    \n    context = raw_result['result']\n    start_index = context.find(\"Helpful Answer:\")\n    extracted_string = context[start_index:].strip().replace(\"Helpful Answer: \" , \"\")\n    print(extracted_string)\n    # print(\" \")\n    # print(\"Sources:\")\n    # src=[]\n    # for ele in raw_result['source_documents']:\n    #     src.append(find_link(ele , df))\n\n    # my_list = list(set(src))\n    # filtered_list = list(filter(lambda x: x is not None, my_list))\n\n\n\n    return { 'query': raw_result['query'] ,'ouput_response':extracted_string  , 'sources': list(df['url']) }\n\n#--------------------------------------------------#\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:18:03.063856Z","iopub.execute_input":"2024-05-13T15:18:03.064132Z","iopub.status.idle":"2024-05-13T15:18:03.070468Z","shell.execute_reply.started":"2024-05-13T15:18:03.064109Z","shell.execute_reply":"2024-05-13T15:18:03.069565Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from langchain.chains import RetrievalQA\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:18:03.071523Z","iopub.execute_input":"2024-05-13T15:18:03.071829Z","iopub.status.idle":"2024-05-13T15:18:03.081828Z","shell.execute_reply.started":"2024-05-13T15:18:03.071805Z","shell.execute_reply":"2024-05-13T15:18:03.081022Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def web_query(query):\n\n    retriever= web_scrap(query)\n#     qa_chain_with_memory = ConversationalRetrievalChain.from_llm(llm,\n#                                             retriever=retriever,\n#                                             memory=memory,\n#                                             verbose=False ,\n#                                             return_source_documents = True\n#                                             )\n\n    qa_chain_without_mem = RetrievalQA.from_chain_type(\n        llm = llm,\n        chain_type = \"stuff\", # map_reduce, map_rerank, stuff, refine\n        retriever = retriever, \n        #chain_type_kwargs = {\"prompt\": PROMPT},\n        return_source_documents = True,\n        verbose = False\n    )\n    result = qa_chain_without_mem(query)\n#     context = result['result']\n#     start_index = context.find(\"Helpful Answer:\")\n#     extracted_string = context[start_index:].strip().replace(\"Helpful Answer: \" , \"\")\n    \n\n    return qa_post_processing_mem(result)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:18:03.092180Z","iopub.execute_input":"2024-05-13T15:18:03.092471Z","iopub.status.idle":"2024-05-13T15:18:03.100391Z","shell.execute_reply.started":"2024-05-13T15:18:03.092449Z","shell.execute_reply":"2024-05-13T15:18:03.099642Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"web_query(\"how to fix Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\")","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:18:03.101460Z","iopub.execute_input":"2024-05-13T15:18:03.101769Z","iopub.status.idle":"2024-05-13T15:18:14.092651Z","shell.execute_reply.started":"2024-05-13T15:18:03.101737Z","shell.execute_reply":"2024-05-13T15:18:14.091647Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Search complete\nCreated embedding\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n  warn_deprecated(\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Answer\nYou can suppress the warning by setting `pad_token_id` to `eos_token_id` when calling `model.generate()`. This is mentioned in a Stack Overflow post and has been confirmed to work fine. However, it would be helpful to have an explanation as to why this works and what the potential implications might be.\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'query': 'how to fix Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.',\n 'ouput_response': 'You can suppress the warning by setting `pad_token_id` to `eos_token_id` when calling `model.generate()`. This is mentioned in a Stack Overflow post and has been confirmed to work fine. However, it would be helpful to have an explanation as to why this works and what the potential implications might be.',\n 'sources': ['https://stackoverflow.com/questions/69609401/suppress-huggingface-logging-warning-setting-pad-token-id-to-eos-token-id',\n  'https://discuss.huggingface.co/t/setting-pad-token-id-to-eos-token-id-50256-for-open-end-generation/22247',\n  'https://github.com/huggingface/transformers/issues/12020',\n  'https://discuss.huggingface.co/t/strange-outputs-in-mixtral-model/73628',\n  'https://www.reddit.com/r/KoboldAI/comments/174k8ry/do_i_need_to_worry_about_this_error_message_and/',\n  'https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/discussions/40',\n  'https://www.reddit.com/r/KoboldAI/comments/nyqlqz/error_received_when_entering_prompt/',\n  'https://jaketae.github.io/study/gpt2/']}"},"metadata":{}}]},{"cell_type":"code","source":"web_query(\"news latest in delhi?\")","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:18:14.094076Z","iopub.execute_input":"2024-05-13T15:18:14.094725Z","iopub.status.idle":"2024-05-13T15:18:21.667001Z","shell.execute_reply.started":"2024-05-13T15:18:14.094692Z","shell.execute_reply":"2024-05-13T15:18:21.666116Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Search complete\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Created embedding\nAnswer\nTo get the latest news in Delhi, you can check out the websites mentioned in the context such as timesofindia.indiatimes.com/delhi or ndtv.com/delhi-news for the most recent updates.\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"{'query': 'news latest in delhi?',\n 'ouput_response': 'To get the latest news in Delhi, you can check out the websites mentioned in the context such as timesofindia.indiatimes.com/delhi or ndtv.com/delhi-news for the most recent updates.',\n 'sources': ['https://indianexpress.com/section/cities/delhi/',\n  'https://timesofindia.indiatimes.com/city/delhi',\n  'https://www.hindustantimes.com/cities/delhi-news',\n  'https://www.ndtv.com/delhi-news',\n  'https://www.delhi.edu/',\n  'https://www.ndtv.com/',\n  'https://www.bbc.com/news/world/asia/india',\n  'https://www.worldbank.org/en/country/india']}"},"metadata":{}}]},{"cell_type":"code","source":"web_query(\"latest python version?\")","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:18:21.668039Z","iopub.execute_input":"2024-05-13T15:18:21.668303Z","iopub.status.idle":"2024-05-13T15:18:35.621542Z","shell.execute_reply.started":"2024-05-13T15:18:21.668282Z","shell.execute_reply":"2024-05-13T15:18:35.620573Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Search complete\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Created embedding\nAnswer\nThe latest stable version of Python as of March 7, 2023, is 3.12.3. However, it's important to note that there is also a prerelease version of Python 3.13 available. The release schedule indicates that Python 3.13 will be officially released on October 1, 2024, and will reach end of support on October 2029. So, if you need a stable version for your project, I would recommend using 3.12.3. But if you want to test out the newest features and are willing to deal with potential bugs, then you might consider trying out the prerelease version of 3.13.\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'query': 'latest python version?',\n 'ouput_response': \"The latest stable version of Python as of March 7, 2023, is 3.12.3. However, it's important to note that there is also a prerelease version of Python 3.13 available. The release schedule indicates that Python 3.13 will be officially released on October 1, 2024, and will reach end of support on October 2029. So, if you need a stable version for your project, I would recommend using 3.12.3. But if you want to test out the newest features and are willing to deal with potential bugs, then you might consider trying out the prerelease version of 3.13.\",\n 'sources': ['https://www.python.org/downloads/',\n  'https://discuss.python.org/t/python-latest-stable-version/24563',\n  'https://www.python.org/doc/versions/',\n  'https://devguide.python.org/versions/',\n  'https://discuss.python.org/t/python-version-compatible-with-windows-xp/21338',\n  'https://www.reddit.com/r/learnpython/comments/qlw4hu/should_i_always_download_the_newest_version_of/',\n  'https://stackoverflow.com/questions/52035673/how-can-i-update-to-the-latest-python-version-using-conda',\n  'https://github.com/conda/conda/issues/8633']}"},"metadata":{}}]},{"cell_type":"code","source":"web_query(\"steps to create chatbot using llm\")","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:18:35.622856Z","iopub.execute_input":"2024-05-13T15:18:35.623170Z","iopub.status.idle":"2024-05-13T15:18:50.388835Z","shell.execute_reply.started":"2024-05-13T15:18:35.623145Z","shell.execute_reply":"2024-05-13T15:18:50.387777Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Search complete\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Created embedding\nAnswer\nTo create a chatbot using LangChain's LLM (Language Model), follow these steps:\n\n1. Import the necessary libraries, such as LangChain's LLMChain and OpenAI for language processing.\n2. Create a chatbot template using a prompt template and ConversationBufferMemory. This will allow the chatbot to generate responses based on user input and store chat history for contextually relevant responses.\n3. Set up the chatbot by instantiating the LLMChain class and utilizing its 'predict()' method to generate a response based on the user's input.\n4. Customize the LLM chatbot using LangChain's finetuning capabilities to meet specific customer needs and preferences.\n5. Test and deploy your chatbot for use in various applications.\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"{'query': 'steps to create chatbot using llm',\n 'ouput_response': \"To create a chatbot using LangChain's LLM (Language Model), follow these steps:\\n\\n1. Import the necessary libraries, such as LangChain's LLMChain and OpenAI for language processing.\\n2. Create a chatbot template using a prompt template and ConversationBufferMemory. This will allow the chatbot to generate responses based on user input and store chat history for contextually relevant responses.\\n3. Set up the chatbot by instantiating the LLMChain class and utilizing its 'predict()' method to generate a response based on the user's input.\\n4. Customize the LLM chatbot using LangChain's finetuning capabilities to meet specific customer needs and preferences.\\n5. Test and deploy your chatbot for use in various applications.\",\n 'sources': ['https://www.reddit.com/r/LocalLLaMA/comments/14jk0m3/what_is_the_best_way_to_create_a_knowledgebase/',\n  'https://medium.com/@alisha3/build-your-first-llm-chatbot-77456438f57b',\n  'https://datasciencedojo.com/blog/llm-chatbot/',\n  'https://mrmaheshrajput.medium.com/how-to-build-an-intelligent-qa-chatbot-on-your-data-with-llm-or-chatgpt-d0009d256dce',\n  'https://realpython.com/build-llm-rag-chatbot-with-langchain/',\n  'https://cobusgreyling.medium.com/bootstrapping-a-chatbot-with-a-large-language-model-93fdf5540a1b',\n  'https://community.openai.com/t/create-question-asking-chatbot-that-collects-user-information/657588',\n  'https://lightning.ai/pages/community/tutorial/how-to-build-a-chatbot-using-open-source-llms-like-llama-2-and-falcon/']}"},"metadata":{}}]},{"cell_type":"code","source":"web_query(\"give list of 20 citation for RAG based chabot\")","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:18:50.390086Z","iopub.execute_input":"2024-05-13T15:18:50.390399Z","iopub.status.idle":"2024-05-13T15:19:25.328476Z","shell.execute_reply.started":"2024-05-13T15:18:50.390375Z","shell.execute_reply":"2024-05-13T15:19:25.327408Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Search complete\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Created embedding\nAnswer\nBased on the context provided, it appears that you are asking for examples of citations for using RAG (Rapid Automated Grading) in a chatbot. Here are twenty examples of potential citations for using RAG in a chatbot context:\n\n1. Rosling, H., & Rosling, A. (2018). Factfulness: Ten Reasons We're Wrong About the World—and Why Things Are Better Than You Think. Flatiron Books. (Contains discussion of using RAG for educational purposes)\n2. Wang, X., Zhang, Y., & Li, J. (2019). Design and Development of a Chatbot System Based on Rapid Automated Grading Technology. Journal of Intelligent Information Systems, 53(1), 147-161.\n3. Kang, S., Lee, J., & Kim, D. (2019). Development of a Chatbot System Using Rapid Automated Grading Technology for English Language Learning. International Journal of Advanced Research in Computer Science and Software Engineering, 11(1), 1-10.\n4. Chen, C.-Y., Chang, T.-W., & Lin, C.-H. (2019). Design and Development of a Chatbot System for Mathematics Education Based on Rapid Automated Grading Technology. Journal of Educational Technology Development and Exchange, 12(1), 1-15.\n5. Li, J., Zhang, Y., & Wang, X. (2019). Design and Implementation of a Chatbot System for Chemistry Education Based on Rapid Automated Grading Technology. Journal of Chemical Education, 96(11), 1788-1793.\n6. Zhang, Y., Wang, X., & Li, J. (2019). Design and Development of a Chatbot System for Physics Education Based on Rapid Automated Grading Technology. Journal of Physics: Conference Series, 1513(1), 012035.\n7. Huang, Y., & Chen, Y. (2019). Design and Development of a Chatbot System for Programming Education Based on Rapid Automated Grading Technology. Journal of Information Technology, 34(3), 36\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"{'query': 'give list of 20 citation for RAG based chabot',\n 'ouput_response': \"Based on the context provided, it appears that you are asking for examples of citations for using RAG (Rapid Automated Grading) in a chatbot. Here are twenty examples of potential citations for using RAG in a chatbot context:\\n\\n1. Rosling, H., & Rosling, A. (2018). Factfulness: Ten Reasons We're Wrong About the World—and Why Things Are Better Than You Think. Flatiron Books. (Contains discussion of using RAG for educational purposes)\\n2. Wang, X., Zhang, Y., & Li, J. (2019). Design and Development of a Chatbot System Based on Rapid Automated Grading Technology. Journal of Intelligent Information Systems, 53(1), 147-161.\\n3. Kang, S., Lee, J., & Kim, D. (2019). Development of a Chatbot System Using Rapid Automated Grading Technology for English Language Learning. International Journal of Advanced Research in Computer Science and Software Engineering, 11(1), 1-10.\\n4. Chen, C.-Y., Chang, T.-W., & Lin, C.-H. (2019). Design and Development of a Chatbot System for Mathematics Education Based on Rapid Automated Grading Technology. Journal of Educational Technology Development and Exchange, 12(1), 1-15.\\n5. Li, J., Zhang, Y., & Wang, X. (2019). Design and Implementation of a Chatbot System for Chemistry Education Based on Rapid Automated Grading Technology. Journal of Chemical Education, 96(11), 1788-1793.\\n6. Zhang, Y., Wang, X., & Li, J. (2019). Design and Development of a Chatbot System for Physics Education Based on Rapid Automated Grading Technology. Journal of Physics: Conference Series, 1513(1), 012035.\\n7. Huang, Y., & Chen, Y. (2019). Design and Development of a Chatbot System for Programming Education Based on Rapid Automated Grading Technology. Journal of Information Technology, 34(3), 36\",\n 'sources': ['https://community.openai.com/t/creating-a-chatbot-using-the-data-stored-in-my-huge-database/245942',\n  'https://azure.microsoft.com/en-us/products/ai-services/ai-bot-service',\n  'https://blogs.nvidia.com/blog/chat-with-rtx-available-now/',\n  'https://www.reddit.com/r/LocalLLaMA/comments/14jk0m3/what_is_the_best_way_to_create_a_knowledgebase/',\n  'https://meta.discourse.org/t/advice-on-a-support-bot-for-a-technical-support-forum-discourse-ai-vs-discourse-chatbot/286349',\n  'https://www.nvidia.com/en-us/ai-on-rtx/chatrtx/',\n  'https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search.html',\n  'https://quickaitutorial.com/autogen-langchian-rag-function-call-super-ai-chabot/']}"},"metadata":{}}]},{"cell_type":"code","source":"web_query(\"what is the rag-based chatbot\")","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:19:25.329943Z","iopub.execute_input":"2024-05-13T15:19:25.330638Z","iopub.status.idle":"2024-05-13T15:19:35.750889Z","shell.execute_reply.started":"2024-05-13T15:19:25.330602Z","shell.execute_reply":"2024-05-13T15:19:35.749848Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Search complete\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Created embedding\nAnswer\nA RAG-based chatbot is a type of conversational AI system that uses Retrieval Augmented Generation (RAG) technology to generate accurate and up-to-date responses by combining the capabilities of pre-trained language models with information retrieval systems. It can access and process large amounts of data from various sources to provide precise answers to user queries.\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"{'query': 'what is the rag-based chatbot',\n 'ouput_response': 'A RAG-based chatbot is a type of conversational AI system that uses Retrieval Augmented Generation (RAG) technology to generate accurate and up-to-date responses by combining the capabilities of pre-trained language models with information retrieval systems. It can access and process large amounts of data from various sources to provide precise answers to user queries.',\n 'sources': ['https://www.anaconda.com/blog/how-to-build-a-retrieval-augmented-generation-chatbot',\n  'https://www.pinecone.io/learn/context-aware-chatbot-with-vercel-ai-sdk/',\n  'https://www.reddit.com/r/LangChain/comments/1agfnox/is_there_any_good_rag_based_opensource_chatbot/',\n  'https://www.k2view.com/blog/rag-chatbot/',\n  'https://www.aporia.com/learn/build-a-rag-chatbot/',\n  'https://www.linkedin.com/pulse/key-metrics-when-implementing-rag-based-generative-ai-kraisingkorn-53nrc',\n  'https://medium.com/credera-engineering/build-a-simple-rag-chatbot-with-langchain-b96b233e1b2a',\n  'https://www.reply.com/en/newsroom/news/storm-reply-launches-rag-based-ai-chatbot-for-audi-revolutionising-internal-documentation']}"},"metadata":{}}]},{"cell_type":"code","source":"web_query(\"latest election news in india\")","metadata":{"execution":{"iopub.status.busy":"2024-05-13T15:19:53.124007Z","iopub.execute_input":"2024-05-13T15:19:53.124719Z","iopub.status.idle":"2024-05-13T15:20:04.678723Z","shell.execute_reply.started":"2024-05-13T15:19:53.124687Z","shell.execute_reply":"2024-05-13T15:20:04.677653Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Search complete\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Created embedding\nAnswer\nAccording to recent news reports, Prime Minister Narendra Modi's Bharatiya Janata Party (BJP) is expected to sweep the upcoming Lok Sabha elections in India, while the Congress party may hit a record low. This information comes from a survey conducted by Reuters. However, it's important to note that elections can be unpredictable and anything can happen until the final votes are counted.\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"{'query': 'latest election news in india',\n 'ouput_response': \"According to recent news reports, Prime Minister Narendra Modi's Bharatiya Janata Party (BJP) is expected to sweep the upcoming Lok Sabha elections in India, while the Congress party may hit a record low. This information comes from a survey conducted by Reuters. However, it's important to note that elections can be unpredictable and anything can happen until the final votes are counted.\",\n 'sources': ['https://www.indiatoday.in/elections',\n  'https://indianexpress.com/elections/',\n  'https://www.reuters.com/world/india/modi-could-sweep-indian-election-congress-may-hit-record-low-says-survey-2024-04-03/',\n  'https://timesofindia.indiatimes.com/elections',\n  'https://www.amnesty.org/en/latest/news/2024/03/india-crackdown-on-opposition-reaches-a-crisis-point-ahead-of-national-elections/',\n  'https://apnews.com/article/india-election-modi-bjp-democracy-8998fe6aba5fa26debc0f82c4e2ccf69',\n  'https://www.bbc.com/news/world/asia/india',\n  'https://eci.gov.in/']}"},"metadata":{}}]},{"cell_type":"code","source":"web_query(\"who is vidipt vashist\")","metadata":{},"execution_count":null,"outputs":[]}]}